{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf46cfcb-b186-4897-8c0f-c4cb08366409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "Dataset loaded successfully!\n",
      "Dataset shape: (200, 5)\n",
      "\n",
      "==================================================\n",
      "FIRST 5 ROWS OF THE DATASET:\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "DATASET INFORMATION:\n",
      "==================================================\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200 entries, 0 to 199\n",
      "Data columns (total 5 columns):\n",
      " #   Column                  Non-Null Count  Dtype \n",
      "---  ------                  --------------  ----- \n",
      " 0   CustomerID              200 non-null    int64 \n",
      " 1   Gender                  200 non-null    object\n",
      " 2   Age                     200 non-null    int64 \n",
      " 3   Annual Income (k$)      200 non-null    int64 \n",
      " 4   Spending Score (1-100)  200 non-null    int64 \n",
      "dtypes: int64(4), object(1)\n",
      "memory usage: 7.9+ KB\n",
      "\n",
      "==================================================\n",
      "BASIC STATISTICS:\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "COLUMN NAMES:\n",
      "==================================================\n",
      "Original columns: ['CustomerID', 'Gender', 'Age', 'Annual Income (k$)', 'Spending Score (1-100)']\n",
      "\n",
      "============================================================\n",
      "DATA QUALITY ASSESSMENT\n",
      "============================================================\n",
      "\n",
      "1. MISSING VALUES:\n",
      "CustomerID                0\n",
      "Gender                    0\n",
      "Age                       0\n",
      "Annual Income (k$)        0\n",
      "Spending Score (1-100)    0\n",
      "dtype: int64\n",
      "Total missing values: 0\n",
      "\n",
      "2. DUPLICATE ROWS:\n",
      "Number of duplicate rows: 0\n",
      "\n",
      "3. DATA TYPES:\n",
      "CustomerID                 int64\n",
      "Gender                    object\n",
      "Age                        int64\n",
      "Annual Income (k$)         int64\n",
      "Spending Score (1-100)     int64\n",
      "dtype: object\n",
      "\n",
      "4. UNIQUE VALUES IN CATEGORICAL COLUMNS:\n",
      "\n",
      "Gender:\n",
      "  Unique values: 2\n",
      "  Values: ['Male' 'Female']\n",
      "\n",
      "5. POTENTIAL OUTLIERS:\n",
      "CustomerID: 0 potential outliers\n",
      "Age: 0 potential outliers\n",
      "Annual Income (k$): 2 potential outliers\n",
      "Spending Score (1-100): 0 potential outliers\n",
      "\n",
      "============================================================\n",
      "STARTING DATA CLEANING PROCESS\n",
      "============================================================\n",
      "\n",
      "3.1 CLEANING COLUMN NAMES:\n",
      "Before: ['CustomerID', 'Gender', 'Age', 'Annual Income (k$)', 'Spending Score (1-100)']\n",
      "After: ['customerid', 'gender', 'age', 'annual_income_k', 'spending_score_1_100']\n",
      "\n",
      "3.2 HANDLING MISSING VALUES:\n",
      "No missing values found!\n",
      "\n",
      "3.3 REMOVING DUPLICATES:\n",
      "No duplicate rows found!\n",
      "\n",
      "3.4 STANDARDIZING TEXT DATA:\n",
      "Cleaned text in column: gender\n",
      "\n",
      "3.5 FIXING DATA TYPES:\n",
      "\n",
      "3.6 HANDLING DATE COLUMNS:\n",
      "No date columns found\n",
      "\n",
      "============================================================\n",
      "POST-CLEANING VALIDATION\n",
      "============================================================\n",
      "1. MISSING VALUES AFTER CLEANING:\n",
      "customerid              0\n",
      "gender                  0\n",
      "age                     0\n",
      "annual_income_k         0\n",
      "spending_score_1_100    0\n",
      "dtype: int64\n",
      "\n",
      "2. DUPLICATE ROWS AFTER CLEANING:\n",
      "Duplicate rows: 0\n",
      "\n",
      "3. DATA TYPES AFTER CLEANING:\n",
      "customerid               int64\n",
      "gender                  object\n",
      "age                      int64\n",
      "annual_income_k          int64\n",
      "spending_score_1_100     int64\n",
      "dtype: object\n",
      "\n",
      "4. FINAL DATASET SHAPE:\n",
      "Original shape: (200, 5)\n",
      "Cleaned shape: (200, 5)\n",
      "\n",
      "============================================================\n",
      "SAVING CLEANED DATASET\n",
      "============================================================\n",
      "Cleaned dataset saved as: cleaned_dataset.csv\n",
      "\n",
      "============================================================\n",
      "DATA CLEANING SUMMARY REPORT\n",
      "============================================================\n",
      "Original dataset: 200 rows, 5 columns\n",
      "Cleaned dataset: 200 rows, 5 columns\n",
      "Rows removed: 0\n",
      "\n",
      "Changes made during cleaning:\n",
      "1. Standardized column names (lowercase, underscores)\n",
      "2. Standardized gender values in gender\n",
      "\n",
      "Final data quality:\n",
      "- Missing values: 0\n",
      "- Duplicate rows: 0\n",
      "- Data types: {dtype('int64'): 4, dtype('O'): 1}\n",
      "\n",
      "============================================================\n",
      "DATA CLEANING COMPLETED SUCCESSFULLY!\n",
      "============================================================\n",
      "\n",
      "Final cleaned dataset (first 10 rows):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customerid</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>annual_income_k</th>\n",
       "      <th>spending_score_1_100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>male</td>\n",
       "      <td>21</td>\n",
       "      <td>15</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>female</td>\n",
       "      <td>23</td>\n",
       "      <td>16</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>female</td>\n",
       "      <td>31</td>\n",
       "      <td>17</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>female</td>\n",
       "      <td>22</td>\n",
       "      <td>17</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>female</td>\n",
       "      <td>35</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>female</td>\n",
       "      <td>23</td>\n",
       "      <td>18</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>male</td>\n",
       "      <td>64</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>female</td>\n",
       "      <td>30</td>\n",
       "      <td>19</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customerid  gender  age  annual_income_k  spending_score_1_100\n",
       "0           1    male   19               15                    39\n",
       "1           2    male   21               15                    81\n",
       "2           3  female   20               16                     6\n",
       "3           4  female   23               16                    77\n",
       "4           5  female   31               17                    40\n",
       "5           6  female   22               17                    76\n",
       "6           7  female   35               18                     6\n",
       "7           8  female   23               18                    94\n",
       "8           9    male   64               19                     3\n",
       "9          10  female   30               19                    72"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Cleaning and Preprocessing - Task 1\n",
    "# Data Analyst Internship\n",
    "\n",
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 1: LOAD AND EXPLORE THE DATASET\n",
    "# =============================================================================\n",
    "\n",
    "# Load the dataset (replace 'dataset.csv' with your actual filename)\n",
    "df = pd.read_csv('Mall_Customers.csv')  # Adjust filename as needed\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FIRST 5 ROWS OF THE DATASET:\")\n",
    "print(\"=\"*50)\n",
    "df.head()\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DATASET INFORMATION:\")\n",
    "print(\"=\"*50)\n",
    "df.info()\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"BASIC STATISTICS:\")\n",
    "print(\"=\"*50)\n",
    "df.describe()\n",
    "\n",
    "# Check column names\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"COLUMN NAMES:\")\n",
    "print(\"=\"*50)\n",
    "print(\"Original columns:\", df.columns.tolist())\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 2: IDENTIFY DATA QUALITY ISSUES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\n1. MISSING VALUES:\")\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values)\n",
    "print(f\"Total missing values: {missing_values.sum()}\")\n",
    "\n",
    "# Check for duplicate rows\n",
    "print(\"\\n2. DUPLICATE ROWS:\")\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicates}\")\n",
    "\n",
    "# Check data types\n",
    "print(\"\\n3. DATA TYPES:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Check unique values for categorical columns\n",
    "print(\"\\n4. UNIQUE VALUES IN CATEGORICAL COLUMNS:\")\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Unique values: {df[col].nunique()}\")\n",
    "    print(f\"  Values: {df[col].unique()}\")\n",
    "\n",
    "# Check for potential outliers in numerical columns\n",
    "print(\"\\n5. POTENTIAL OUTLIERS:\")\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "for col in numerical_cols:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "    print(f\"{col}: {len(outliers)} potential outliers\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 3: DATA CLEANING OPERATIONS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STARTING DATA CLEANING PROCESS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create a copy of the original dataset for cleaning\n",
    "df_original = df.copy()\n",
    "df_cleaned = df.copy()\n",
    "\n",
    "# Track changes\n",
    "changes_made = []\n",
    "\n",
    "# 3.1 Clean Column Names\n",
    "print(\"\\n3.1 CLEANING COLUMN NAMES:\")\n",
    "print(\"Before:\", df_cleaned.columns.tolist())\n",
    "\n",
    "# Standardize column names: lowercase, replace spaces with underscores\n",
    "df_cleaned.columns = df_cleaned.columns.str.lower().str.replace(' ', '_').str.replace('-', '_')\n",
    "df_cleaned.columns = df_cleaned.columns.str.replace('[^a-zA-Z0-9_]', '', regex=True)\n",
    "\n",
    "print(\"After:\", df_cleaned.columns.tolist())\n",
    "changes_made.append(\"Standardized column names (lowercase, underscores)\")\n",
    "\n",
    "# 3.2 Handle Missing Values\n",
    "print(\"\\n3.2 HANDLING MISSING VALUES:\")\n",
    "if df_cleaned.isnull().sum().sum() > 0:\n",
    "    print(\"Missing values found, handling them...\")\n",
    "    \n",
    "    # For numerical columns: fill with median\n",
    "    for col in df_cleaned.select_dtypes(include=[np.number]).columns:\n",
    "        if df_cleaned[col].isnull().sum() > 0:\n",
    "            median_value = df_cleaned[col].median()\n",
    "            df_cleaned[col].fillna(median_value, inplace=True)\n",
    "            changes_made.append(f\"Filled missing values in {col} with median ({median_value})\")\n",
    "    \n",
    "    # For categorical columns: fill with mode\n",
    "    for col in df_cleaned.select_dtypes(include=['object']).columns:\n",
    "        if df_cleaned[col].isnull().sum() > 0:\n",
    "            mode_value = df_cleaned[col].mode()[0] if len(df_cleaned[col].mode()) > 0 else 'Unknown'\n",
    "            df_cleaned[col].fillna(mode_value, inplace=True)\n",
    "            changes_made.append(f\"Filled missing values in {col} with mode ({mode_value})\")\n",
    "else:\n",
    "    print(\"No missing values found!\")\n",
    "\n",
    "# 3.3 Remove Duplicates\n",
    "print(\"\\n3.3 REMOVING DUPLICATES:\")\n",
    "if duplicates > 0:\n",
    "    df_cleaned = df_cleaned.drop_duplicates()\n",
    "    print(f\"Removed {duplicates} duplicate rows\")\n",
    "    changes_made.append(f\"Removed {duplicates} duplicate rows\")\n",
    "else:\n",
    "    print(\"No duplicate rows found!\")\n",
    "\n",
    "# 3.4 Standardize Text Data\n",
    "print(\"\\n3.4 STANDARDIZING TEXT DATA:\")\n",
    "for col in df_cleaned.select_dtypes(include=['object']).columns:\n",
    "    # Remove leading/trailing whitespace and standardize case\n",
    "    df_cleaned[col] = df_cleaned[col].astype(str).str.strip()\n",
    "    \n",
    "    # If it's a gender-like column, standardize values\n",
    "    if 'gender' in col.lower():\n",
    "        df_cleaned[col] = df_cleaned[col].str.lower()\n",
    "        df_cleaned[col] = df_cleaned[col].replace({\n",
    "            'm': 'male', 'f': 'female', 'male ': 'male', 'female ': 'female',\n",
    "            'man': 'male', 'woman': 'female'\n",
    "        })\n",
    "        changes_made.append(f\"Standardized gender values in {col}\")\n",
    "    \n",
    "    print(f\"Cleaned text in column: {col}\")\n",
    "\n",
    "# 3.5 Fix Data Types\n",
    "print(\"\\n3.5 FIXING DATA TYPES:\")\n",
    "# This section will depend on your specific dataset\n",
    "# Common fixes:\n",
    "for col in df_cleaned.columns:\n",
    "    if 'id' in col.lower() and df_cleaned[col].dtype == 'float64':\n",
    "        df_cleaned[col] = df_cleaned[col].astype('int64')\n",
    "        changes_made.append(f\"Converted {col} to integer\")\n",
    "    elif 'age' in col.lower() and df_cleaned[col].dtype == 'float64':\n",
    "        df_cleaned[col] = df_cleaned[col].astype('int64')\n",
    "        changes_made.append(f\"Converted {col} to integer\")\n",
    "\n",
    "# 3.6 Handle Date Columns (if any exist)\n",
    "print(\"\\n3.6 HANDLING DATE COLUMNS:\")\n",
    "date_columns = [col for col in df_cleaned.columns if 'date' in col.lower() or 'time' in col.lower()]\n",
    "if date_columns:\n",
    "    for col in date_columns:\n",
    "        try:\n",
    "            df_cleaned[col] = pd.to_datetime(df_cleaned[col])\n",
    "            changes_made.append(f\"Converted {col} to datetime format\")\n",
    "            print(f\"Converted {col} to datetime\")\n",
    "        except:\n",
    "            print(f\"Could not convert {col} to datetime\")\n",
    "else:\n",
    "    print(\"No date columns found\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 4: QUALITY VALIDATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"POST-CLEANING VALIDATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check for remaining issues\n",
    "print(\"1. MISSING VALUES AFTER CLEANING:\")\n",
    "print(df_cleaned.isnull().sum())\n",
    "\n",
    "print(\"\\n2. DUPLICATE ROWS AFTER CLEANING:\")\n",
    "print(f\"Duplicate rows: {df_cleaned.duplicated().sum()}\")\n",
    "\n",
    "print(\"\\n3. DATA TYPES AFTER CLEANING:\")\n",
    "print(df_cleaned.dtypes)\n",
    "\n",
    "print(\"\\n4. FINAL DATASET SHAPE:\")\n",
    "print(f\"Original shape: {df_original.shape}\")\n",
    "print(f\"Cleaned shape: {df_cleaned.shape}\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 5: SAVE CLEANED DATASET\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAVING CLEANED DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save the cleaned dataset\n",
    "cleaned_filename = 'cleaned_dataset.csv'\n",
    "df_cleaned.to_csv(cleaned_filename, index=False)\n",
    "print(f\"Cleaned dataset saved as: {cleaned_filename}\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 6: SUMMARY REPORT\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA CLEANING SUMMARY REPORT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"Original dataset: {df_original.shape[0]} rows, {df_original.shape[1]} columns\")\n",
    "print(f\"Cleaned dataset: {df_cleaned.shape[0]} rows, {df_cleaned.shape[1]} columns\")\n",
    "print(f\"Rows removed: {df_original.shape[0] - df_cleaned.shape[0]}\")\n",
    "\n",
    "print(\"\\nChanges made during cleaning:\")\n",
    "for i, change in enumerate(changes_made, 1):\n",
    "    print(f\"{i}. {change}\")\n",
    "\n",
    "print(\"\\nFinal data quality:\")\n",
    "print(f\"- Missing values: {df_cleaned.isnull().sum().sum()}\")\n",
    "print(f\"- Duplicate rows: {df_cleaned.duplicated().sum()}\")\n",
    "print(f\"- Data types: {dict(df_cleaned.dtypes.value_counts())}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA CLEANING COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Display final cleaned dataset sample\n",
    "print(\"\\nFinal cleaned dataset (first 10 rows):\")\n",
    "df_cleaned.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ee94b0-e44d-43f2-8b18-dae3824769f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
